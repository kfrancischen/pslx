{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Package PSLX \u00b6 P ython S tandard L ibrary e X tension. A standard library for job scheduling, micro services including message queue, RPC, instant messaging and monitoring, tooling such as logging, file storage and caching. The library is written compatible to Python 3.7+. To use the program, please install the latest protobuf compiler and gRPC tools. This document covers the APIs of PSLX provided, and is organized as follows: Websocket \u00b6 Base class for Websocket , including abstract class for connection and interface for message parsing. Job Scheduling \u00b6 Running ad-hoc and scheduled jobs with operators and operator containers . Data Storage \u00b6 Data storage using plain text file, in-memory fixed size file, proto table, and timestamp-based partitioner . RPC \u00b6 General framework of RPC used in PSLX. Micro Services \u00b6 Instant messaging services built for Slack, Rocketchat and Microsoft Teams using the PSLX RPC framework. Email service for sending emails through code using the PSLX RPC framework. RPC storage io for reading the data storage through remote RPC calls using the PSLX RPC framework. Message Queue for building simple message queue using proto buffers. Part of the implementation also is shared with the PSLX rpc framework. Publisher/subscriber for building simple applications with pubsub model. PSLX Frontend Flask frontend built to monitor the health of RPC servers, container status, and browser the content of protobufs and file system for local and remote servers. Tools \u00b6 Tools provided by PSLX for thread-safe file io, LRC caching, ttl-ed logging, SQL server read/write, Mongodb tool, timestamp key-ed partitioner local/remote fetcher and watcher, function registration. Utilities \u00b6 Utilities provided by PSLX for file operation, protobuf related utilities, async unittesting, timezone related utilities, yaml file io, text coloring, and credential composing, environment variables access and decorators. TTL Cleaner \u00b6 Internally built ttl_cleaner for temporary/ttl-ed file removing and garbage collection. Proto Schema \u00b6 Internal protobuf schemas . Please also take a look at the example folder for different example implementations.","title":"Home"},{"location":"#package-pslx","text":"P ython S tandard L ibrary e X tension. A standard library for job scheduling, micro services including message queue, RPC, instant messaging and monitoring, tooling such as logging, file storage and caching. The library is written compatible to Python 3.7+. To use the program, please install the latest protobuf compiler and gRPC tools. This document covers the APIs of PSLX provided, and is organized as follows:","title":"Package PSLX"},{"location":"#websocket","text":"Base class for Websocket , including abstract class for connection and interface for message parsing.","title":"Websocket"},{"location":"#job-scheduling","text":"Running ad-hoc and scheduled jobs with operators and operator containers .","title":"Job Scheduling"},{"location":"#data-storage","text":"Data storage using plain text file, in-memory fixed size file, proto table, and timestamp-based partitioner .","title":"Data Storage"},{"location":"#rpc","text":"General framework of RPC used in PSLX.","title":"RPC"},{"location":"#micro-services","text":"Instant messaging services built for Slack, Rocketchat and Microsoft Teams using the PSLX RPC framework. Email service for sending emails through code using the PSLX RPC framework. RPC storage io for reading the data storage through remote RPC calls using the PSLX RPC framework. Message Queue for building simple message queue using proto buffers. Part of the implementation also is shared with the PSLX rpc framework. Publisher/subscriber for building simple applications with pubsub model. PSLX Frontend Flask frontend built to monitor the health of RPC servers, container status, and browser the content of protobufs and file system for local and remote servers.","title":"Micro Services"},{"location":"#tools","text":"Tools provided by PSLX for thread-safe file io, LRC caching, ttl-ed logging, SQL server read/write, Mongodb tool, timestamp key-ed partitioner local/remote fetcher and watcher, function registration.","title":"Tools"},{"location":"#utilities","text":"Utilities provided by PSLX for file operation, protobuf related utilities, async unittesting, timezone related utilities, yaml file io, text coloring, and credential composing, environment variables access and decorators.","title":"Utilities"},{"location":"#ttl-cleaner","text":"Internally built ttl_cleaner for temporary/ttl-ed file removing and garbage collection.","title":"TTL Cleaner"},{"location":"#proto-schema","text":"Internal protobuf schemas . Please also take a look at the example folder for different example implementations.","title":"Proto Schema"},{"location":"container/","text":"PSLX built-in container implements a DAG connected graph structure . For each container (graph), the node is defined as an operator, and therefore, each operator contains a dictionary of its children operators and parent operators. The execute flow of the container will then start from the root and finish after all the leaf operators are done. Each operator defined in a container will have status of Status.IDLE , Status.WAITING , Status.RUNNING , Status.SUCCEEDED and Status.FAILED . The operator will be in Status.IDLE if its parents are not waiting for being executed, and will be in Status.WAITING if its parents in the process of being executed now. Then it will turn to the status of Status.RUNNING when all its parents are done executing, and its final status could be one of Status.SUCCEEDED and Status.FAILED depending on the execution. One failure in the container graph will result in failure in all the subsequent operators. PSLX has two kinds of data models: DataModelType.BATCH and DataModelType.STREAMING , and the enums are defined as protobuf enums (Please check the section of schema ). DataModelType.BATCH mode supports running operators in multi-thread fashion, and allows each operator to send its status to the backend, while DataModelType.STREAMING only supports sending status to the backend when the container starts and ends. For the execution, DataModelType.STREAMING allows operators to fail in the middle, while for DataModelType.BATCH , one failure in an operator will result in the failure of the overall container. The two types of operations all have the following functions: Operator Documentation \u00b6 Note The base implementation of operator is in operator_base.py , and the ones for the DataModelType.BATCH and DataModelType.STREAMING are in batch/operator.py and streaming/operator.py , respectively. __init__ ( operator_name , logger ) Description: Construct an operator. Arguments: operator_name: the name of the operator. logger: the logger for the operator. set_data_model ( model ) Description: change the data model to a different model. Arguments: model: the new data model, one of DataModelType.BATCH and DataModelType.STREAMING . unset_data_model () Description: Unset the preset data model to DataModelType.DEFAULT . get_data_model () Description: Get the model of the operator. Return: data model of the operator, one of DataModelType.BATCH , DataModelType.STREAMING and DataModelType.DEFAULT . set_status ( status ) Description: Set the status of the operator. Arguments: status: the new status, one of Status.IDLE , Status.WAITING , Status.RUNNING , Status.SUCCEEDED and Status.FAILED . unset_status () Description: Unset the status to Status.IDLE . unset_dependency () Description: Remove all the dependencies related to this operator, including removing children and parents, and the parents and children also will remove the operator. get_status () Description: Get the status of the operator. Return: the status of the operator, one of Status.IDLE , Status.WAITING , Status.RUNNING , Status.SUCCEEDED and Status.FAILED . counter_increment ( counter_name ) Description: increment the counter name by 1. Arguments: counter_name: the name of the counter. The final counter name would be in the format of operator_name:counter_name . counter_increment_by_n ( counter_name , n ) Description: increment the counter name by n (n > 0). Arguments: counter_name: the name of the counter. The final counter name would be in the format of operator_name:counter_name . n: the increment amount. mark_as_done () Description: Mark the status of the operator as Status.SUCCEEDED . mark_as_persistent () Description: Mark the content of the operator to be persistent, and if true, the content will be stored in the snapshot. get_content () Description: Get the content contained in the operator. Return: the content of the operator if the operator is persistent, otherwise None. set_content ( content ) Description: Set the content of the operator. Arguments: content: the new content, needed to be a protobuf message whose type can be any user defined one. is_done () Description: Check whether the operator status is Status.SUCCEEDED . Return: True if the operator status is Status.SUCCEEDED , otherwise False. get_content_from_dependency ( dependency_name ) Description: Get the content from the dependency with name equal to the dependency name. Arguments: dependency_name: the name of the dependency. Return: Proto message contained in the operator with dependency name. get_content_from_snapshot ( snapshot_file , message_type ): Description: Get the content from a snapshot file. Arguments: snapshot_file: the file name of the operator snapshot file. message_type: the type of the content message. Return: Proto message contained in the operator snapshot file in the format of the given message type. get_status_from_snapshot ( operator_name ) Description: Get the status from a snapshot file. Arguments: snapshot_file: the file name of the operator snapshot file. Return: Status contained in the operator snapshot file. wait_for_upstream_status () Description: Get the upstream unfinished operator names. Return: A list of unfinished operator names from upstream. is_data_model_consistent () Description: Check whether the models of the children and parents and the model of self are consistent. Return: True if consistent, otherwise False. is_status_consistent ( operator_name , order = SortOrder . ORDER ) Description: Check whether the status of the children and parents and the status of self are consistent. Return: True if consistent, otherwise False. execute () Description: Execute the operation implemented in the operator. set_config ( config ) Description: Set the configuration of the operator. The built-in config will then add the config, and this function can be used as an entry point to control parameters in the operator. Arguments: config: a dictionary containing key-value configurations. One common argument is save_snapshot , and it is true, the snapshot of the operator will be saved, and vice versa. The default value for this argument is False. Info Function that needs to be implemented by any client. execute_impl () Description: Implement the operators conducted by the operator. Container Documentation \u00b6 Note The base implementation of container is in contain_base.py , and the ones for the DataModelType.BATCH and DataModelType.STREAMING are in batch/container.py and streaming/container.py , respectively. __init__ ( container_name , ttl ) Description: Construct an container with name and ttl. Note that please make sure all the container names are unique across your application. Arguments: container_name: name of the container. ttl: the ttl policy. It could be a string ending with m (for minutes), h (for hours) and d (for days). Negative ttl means the container's temporary files will be permanent. bind_backend ( server_url , root_certificate = None ) Description: Binds to the container backend, described in the frontend section. Arguments: server_url: the url to the rpc server. root_certificate: the root certificate of the host of rpc server, default to None. initialize ( force = False ) Description: Initialize the container. Once a container is initialized, it is then ready for execution. An uninitialized container cannot be executed. Arguments: force: whether to force the status and data model of its contained operators to be consistent with self. set_status ( status ) Description: Set the status of the container. Arguments: status: the new status, one of Status.IDLE , Status.WAITING , Status.RUNNING , Status.SUCCEEDED and Status.FAILED . unset_status () Description: Unset the status to Status.IDLE . uninitialize () Description: Uninitialize the container. add_operator_edge ( from_operator , to_operator ) Description: Add a dependency edge from a from_operator to a to_operator in the container. Arguments: from_operator: the upstream operator. to_operator: the downstream operator. add_upstream_op ( op_snapshot_file_pattern ) Description: Add an upstream operator outside the container. The container then will monitor the change of the snapshot of the operator until the status becomes Status.SUCCEEDED . Arguments: op_snapshot_file_pattern: the snapshot file pattern of the upstream operator. execute ( is_backfill , num_threads ) Description: Execute the container graph. Arguments: is_backfill: whether the execution runs in backfill mode. If so, the successful operators will be executed again. num_threads: number of threads used for the execution. If the container is of type DataModelType.STREAMING , this input will be ignored and only one thread is allowed. Note The following function is only for CronBatchContainer and IntervalBatchContainer . execute_now ( is_backfill , num_threads ) Description: Execute the container graph now for once. Then there is not wait for the schedule. Arguments: is_backfill: whether the execution runs in backfill mode. If so, the successful operators will be executed again. num_threads: number of threads used for the execution. Also for CronBatchContainer and CronStreamingContainer , we do have add_schedule ( day_of_week , hour , minute = None , second = None , misfire_grace_time = None ) Description: Add cron like schedule to the container. There could be multiple if schedules. Arguments: please check the related arguments in apscheduler . For IntervalBatchContainer and IntervalStreamingContainer , we do have add_schedule ( days , hours = 0 , minutes = 0 , seconds = 0 , misfire_grace_time = None ) Description: Add interval schedule to the container. There could be multiple if schedules. Arguments: please check the related arguments in apscheduler . In addition, there exist two other containers NonStoppingBatchContainer and NonStoppingStreamingContainer that will continously execute again immediately after one successful execution.","title":"Job Scheduling"},{"location":"container/#operator-documentation","text":"Note The base implementation of operator is in operator_base.py , and the ones for the DataModelType.BATCH and DataModelType.STREAMING are in batch/operator.py and streaming/operator.py , respectively. __init__ ( operator_name , logger ) Description: Construct an operator. Arguments: operator_name: the name of the operator. logger: the logger for the operator. set_data_model ( model ) Description: change the data model to a different model. Arguments: model: the new data model, one of DataModelType.BATCH and DataModelType.STREAMING . unset_data_model () Description: Unset the preset data model to DataModelType.DEFAULT . get_data_model () Description: Get the model of the operator. Return: data model of the operator, one of DataModelType.BATCH , DataModelType.STREAMING and DataModelType.DEFAULT . set_status ( status ) Description: Set the status of the operator. Arguments: status: the new status, one of Status.IDLE , Status.WAITING , Status.RUNNING , Status.SUCCEEDED and Status.FAILED . unset_status () Description: Unset the status to Status.IDLE . unset_dependency () Description: Remove all the dependencies related to this operator, including removing children and parents, and the parents and children also will remove the operator. get_status () Description: Get the status of the operator. Return: the status of the operator, one of Status.IDLE , Status.WAITING , Status.RUNNING , Status.SUCCEEDED and Status.FAILED . counter_increment ( counter_name ) Description: increment the counter name by 1. Arguments: counter_name: the name of the counter. The final counter name would be in the format of operator_name:counter_name . counter_increment_by_n ( counter_name , n ) Description: increment the counter name by n (n > 0). Arguments: counter_name: the name of the counter. The final counter name would be in the format of operator_name:counter_name . n: the increment amount. mark_as_done () Description: Mark the status of the operator as Status.SUCCEEDED . mark_as_persistent () Description: Mark the content of the operator to be persistent, and if true, the content will be stored in the snapshot. get_content () Description: Get the content contained in the operator. Return: the content of the operator if the operator is persistent, otherwise None. set_content ( content ) Description: Set the content of the operator. Arguments: content: the new content, needed to be a protobuf message whose type can be any user defined one. is_done () Description: Check whether the operator status is Status.SUCCEEDED . Return: True if the operator status is Status.SUCCEEDED , otherwise False. get_content_from_dependency ( dependency_name ) Description: Get the content from the dependency with name equal to the dependency name. Arguments: dependency_name: the name of the dependency. Return: Proto message contained in the operator with dependency name. get_content_from_snapshot ( snapshot_file , message_type ): Description: Get the content from a snapshot file. Arguments: snapshot_file: the file name of the operator snapshot file. message_type: the type of the content message. Return: Proto message contained in the operator snapshot file in the format of the given message type. get_status_from_snapshot ( operator_name ) Description: Get the status from a snapshot file. Arguments: snapshot_file: the file name of the operator snapshot file. Return: Status contained in the operator snapshot file. wait_for_upstream_status () Description: Get the upstream unfinished operator names. Return: A list of unfinished operator names from upstream. is_data_model_consistent () Description: Check whether the models of the children and parents and the model of self are consistent. Return: True if consistent, otherwise False. is_status_consistent ( operator_name , order = SortOrder . ORDER ) Description: Check whether the status of the children and parents and the status of self are consistent. Return: True if consistent, otherwise False. execute () Description: Execute the operation implemented in the operator. set_config ( config ) Description: Set the configuration of the operator. The built-in config will then add the config, and this function can be used as an entry point to control parameters in the operator. Arguments: config: a dictionary containing key-value configurations. One common argument is save_snapshot , and it is true, the snapshot of the operator will be saved, and vice versa. The default value for this argument is False. Info Function that needs to be implemented by any client. execute_impl () Description: Implement the operators conducted by the operator.","title":"Operator Documentation"},{"location":"container/#container-documentation","text":"Note The base implementation of container is in contain_base.py , and the ones for the DataModelType.BATCH and DataModelType.STREAMING are in batch/container.py and streaming/container.py , respectively. __init__ ( container_name , ttl ) Description: Construct an container with name and ttl. Note that please make sure all the container names are unique across your application. Arguments: container_name: name of the container. ttl: the ttl policy. It could be a string ending with m (for minutes), h (for hours) and d (for days). Negative ttl means the container's temporary files will be permanent. bind_backend ( server_url , root_certificate = None ) Description: Binds to the container backend, described in the frontend section. Arguments: server_url: the url to the rpc server. root_certificate: the root certificate of the host of rpc server, default to None. initialize ( force = False ) Description: Initialize the container. Once a container is initialized, it is then ready for execution. An uninitialized container cannot be executed. Arguments: force: whether to force the status and data model of its contained operators to be consistent with self. set_status ( status ) Description: Set the status of the container. Arguments: status: the new status, one of Status.IDLE , Status.WAITING , Status.RUNNING , Status.SUCCEEDED and Status.FAILED . unset_status () Description: Unset the status to Status.IDLE . uninitialize () Description: Uninitialize the container. add_operator_edge ( from_operator , to_operator ) Description: Add a dependency edge from a from_operator to a to_operator in the container. Arguments: from_operator: the upstream operator. to_operator: the downstream operator. add_upstream_op ( op_snapshot_file_pattern ) Description: Add an upstream operator outside the container. The container then will monitor the change of the snapshot of the operator until the status becomes Status.SUCCEEDED . Arguments: op_snapshot_file_pattern: the snapshot file pattern of the upstream operator. execute ( is_backfill , num_threads ) Description: Execute the container graph. Arguments: is_backfill: whether the execution runs in backfill mode. If so, the successful operators will be executed again. num_threads: number of threads used for the execution. If the container is of type DataModelType.STREAMING , this input will be ignored and only one thread is allowed. Note The following function is only for CronBatchContainer and IntervalBatchContainer . execute_now ( is_backfill , num_threads ) Description: Execute the container graph now for once. Then there is not wait for the schedule. Arguments: is_backfill: whether the execution runs in backfill mode. If so, the successful operators will be executed again. num_threads: number of threads used for the execution. Also for CronBatchContainer and CronStreamingContainer , we do have add_schedule ( day_of_week , hour , minute = None , second = None , misfire_grace_time = None ) Description: Add cron like schedule to the container. There could be multiple if schedules. Arguments: please check the related arguments in apscheduler . For IntervalBatchContainer and IntervalStreamingContainer , we do have add_schedule ( days , hours = 0 , minutes = 0 , seconds = 0 , misfire_grace_time = None ) Description: Add interval schedule to the container. There could be multiple if schedules. Arguments: please check the related arguments in apscheduler . In addition, there exist two other containers NonStoppingBatchContainer and NonStoppingStreamingContainer that will continously execute again immediately after one successful execution.","title":"Container Documentation"},{"location":"rpc/","text":"RPC is a core component, and micro services built inside PSLX (for example instant messaging , email , rpc storage io ) are concrete implementations of RPC. To build an RPC service using PSLX, one first needs to take a look at the abstract class implementation defined in rpc_base . The following functions are provided __init__ ( service_name , rpc_storage ) Description: Create an RPC instance with given name and underlying storage. Arguments: service_name: the name of the rpc service. Needs to be unique. rpc_storage: the partitioner storage for storing the GenericRPCRequestResponsePair . If None, the GenericRPCRequestResponsePair won't be stored. Info Function needs to be implemented by specific application. get_response_and_status_impl ( request ) Description: get corresponding response and status from the request. Arguments: request: a proto message containing user defined request information. Explanation: The request and response can be any proto message type user defines. A concrete implementation of RPCBase needs to specify the type of request by setting REQUEST_MESSAGE_TYPE as a class level variable. In addition to inheriting and implementing a RPC class, one also needs to create a client class inheriting client_base . The client_base.py provides the following functions __init__ ( client_name , server_url ) Description: Create an client instance. Arguments: client_name: the name of the rpc client. server_url: the url to the rpc server. send_request ( request , root_certificate ) Description: Send request to the server and get response. Arguments: request: the request proto message. root_certificate: the root certificate for the SSL encryption. None if the rpc channel is insecure. The user of PSLX can also define other function to create a specific request and call send_request inside the function. One example can be found at instant_messaging/client.py . In this example, a customized function send_message wraps the send_request and then allows user to pass more concrete arguments to interact with rpc server. Inside PSLX, RPC requests and responses will finally be parsed as Any proto message and hence the interface is universal. To create a PSLX server, the user of PSLX needs to bind an RPC to a server using the methods provided by generic_server . _init__ ( server_name ) Description: Create an generic server instance. Arguments: server_name: the name of the server. Needs to be unique. create_server ( max_worker , server_url ) Description: Create an generic server. Arguments: max_worker: the maximum number of workers for the server. server_url: the url to the rpc server. Note Each generic server is only allowed to bind to one rpc instance. bind_rpc ( rpc ) Description: Bind an RPC instance to the server. Arguments: rpc: the rpc instance to be binded. start_server ( private_key , certificate_chain ) Description: Start running the server. Arguments: private_key: the private key of the SSL encryption. certificate_chain: the certificate chain of the SSL encryption.","title":"RPC"},{"location":"schema/","text":"PSLX uses protobuf to store data. All the schemas are in the folder of schema . In the enums.proto , the following enum types are defined: ModeType : whether it is in test mode or production mode. The mode is controlled by the environment variable named PSLX_TEST , and if in test mode, all the auto generated file will have a TEST/ in their path, otherwise it would be PROD/ . SortOrder : the order of the children nodes and parent nodes in a node. DataModelType : the data model type for the containers. InstantMessagingType : the type of instant messaging app. DiskLoggerLevel : the level of logging, similar to the regular logging level. StorageType : the type of the storage. ReadRuleType : the read rule for DefaultStorage . WriteRuleType : the write rule for DefaultStorage and FixedSizeStorage . PartitionerStorageType : the type of the partitioner. Status : status of the operator, container, and RPC response. Signal : signal for internal usage only. In rpc.proto , RPC-related message types are defined: GenericRPCService : the generic rpc service supported in PSLX. HealthCheckerRequest : the health checker request. HealthCheckerResponse : the health checker response. GenericRPCRequest : generic request that encapsulates the request message in its Any field. GenericRPCResponse : generic response that encapsulates the response message in its Any field. GenericRPCRequestResponsePair : pair of generic request and response. InstantMessagingRPCRequest : instant messaging rpc request. EmailPRCRequest : email rpc request. RPCIOResponse : RPC io rpc request. ProtoViewerRPCRequest : request for proto viewer service. ProtoViewerRPCResponse : response from proto viewer service. FileViewerRPCRequest : request for file viewer service. FileViewerRPCResponse :response from file viewer service. In snapshots.proto , the snapshots for node, operator and container are defined: NodeSnapshot : snapshot for node . OperatorSnapshot : snapshot for operator. ContainerSnapshot : snapshot for container. OperatorContentPlain : a plain text representation for content in operator. OperatorContentList : a list representation for content in operator. 6: OperatorContentDict : a dictionary representation for content in operator In storage.proto , the proto table storage type and the value for the container backend storage are defined: ProtoTable : generic storage schema for ProtoTableStorage . ContainerBackendValue : the value type for container backend service. In common.proto , various credential formats are defined: EmptyMessage : a dummy holder for empty message. Credentials : a specifically designed message to storage credentials. FrontendConfig : the config for frontend.","title":"Proto Schema"},{"location":"storage/","text":"PSLX supports in total five different types of storage: StorageType.DEFAULT_STORAGE , StorageType.FIXED_SIZE_STORAGE , StorageType.PROTO_TABLE_STORAGE , StorageType.SHARDED_PROTO_TABLE_STORAGE and StorageType.PARTITIONER_STORAGE , and the StorageType.PARTITIONER_STORAGE also support five different types of timestamp based partitions: PartitionerStorageType.MINUTELY , PartitionerStorageType.HOURLY , PartitionerStorageType.DAILY , PartitionerStorageType.MONTHLY , PartitionerStorageType.YEARLY . The related enums are defined in schema , and their implementations are in the storage folder. The four stage all inherit from a parent class and storage_base.py , where each inheritance needs to implement its own read and write functions. Besides these two functions, there are a few functions that are shard across all storage types. __init__ ( logger = None ) Description: Construct a storage. Arguments: logger: the logging tool (see tool ) for this storage. Default value is None. set_config ( config ) Description: Updates the initial config. Arguments: config: the config that is added to the existing config. get_storage_type () Description: Get the storage type of the storage. Return: the storage type of the storage, one of StorageType.DEFAULT_STORAGE , StorageType.FIXED_SIZE_STORAGE , StorageType.PROTO_TABLE_STORAGE and StorageType.PARTITIONER_STORAGE . initialize_from_file ( file_name ) Description: initialize the storage from a file, only supported by StorageType.DEFAULT_STORAGE , StorageType.FIXED_SIZE_STORAGE , and StorageType.PROTO_TABLE_STORAGE . Arguments: file_name: the file that is used to initialize the storage. initialize_from_dir ( dir_name ) Description: initialize the storage from a directory, only supported by StorageType.PARTITIONER_STORAGE . Arguments: dir_name: the directory name that is used to initialize the storage. Default Storage Documentation \u00b6 read ( params ) Description: Read from the storage. Arguments: params: the read parameters. Explanation: The underlying file needs to be a text file, and the default storage supports read/write from both top and down (see the definition of ReadRuleType and WriteRuleType in schema ), and can be set by function set_config(config) by overwriting string read_rule_type and write_rule_type with the correct enum. The default values for them are ReadRuleType.READ_FROM_BEGINNING and WriteRuleType.WRITE_FROM_END . The params in the read(params) function supports the number of lines to read from the file. The way to set this field is to pass num_line to the param (a dictionary). If the num_line exceeds the total number of lines in the file, an error will be raised. After reading the underlying file, the file handler will move accordingly. For example, after reading one line from the file, the second time the storage will start by reading the second line of the file. The can be reset by calling start_from_first_line() . write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: If the data is a string, it will be written to the underlying file from top or bottom depending on the value of write_rule_type . If the data is a list, it will be joined with delimter set in the params. If key delimiter is not present in params, comma will be used by default. start_from_first_line () Description: Reset the reader to read from the first line (from top or bottom). Fixed Size Storage Documentation \u00b6 __init__ ( logger = None , fixed_size =- 1 ) Description: Overrides the default constructor. Arguments: logger: the logging tool (see tool ) for this storage. Default value is None. fixed_size: the maximum data size that this storage will hold in memory, negative meaning the maximum size is infinity. read ( params ) Description: Read from the storage. Arguments: params: the read parameters. Explanation: Like the default storage, the underlying file needs to be a text file. The difference between fixed size storage and default storage is that fixed size storage will hold data in memory (while default storage will by default read the underlying file). The params supported here are num_line and force_load . num_line indicates the number of lines to read. If force_load if False and the num_lines exceeds the internal maximum size, error will be raised. If force_load is true, the storage will search for the file. Due to the nature of the fixed size storage, it could be used a buffer storage between disk and application. write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: Same as the writer implementation of default storage. Proto Table Storage \u00b6 read ( params ) Description: Read from the storage. Arguments: params: the read parameters. Explanation: proto table is a key-value storage, and therefore the params need to contain field of key . The value of the proto table is a proto message, and if the field of message_type is provided, the reader will correctly deserialize to the desired protobuf. Otherwise it will only an Any type message. Under PSLX convention, the underlying file name needs to end with .pb . read_all () Description: Read all the data. Return: the key, value dictionary of the table, with value being the Any proto format. write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: The data needs to be a dictionary of key, value with key being a string and value being a protobuf message of any user defined types. The params can contain overwrite with its value a boolean indicating whether to overwrite the value if the key already exists in the proto table. delete ( key ) Description: Delete key and the corresponding entry from the proto table. Arguments: key: the key of the entry to be deleted. delete_all () Description: Delete all the contents from the proto table. Sharded Proto Table Storage \u00b6 Sharded proto table storage will shard the data into different proto tables, denoted by data@SHARD.pb , where the SHARD is an integer that starts from 0 . In addition to these tables, there also exists a index_map.pb protobuf that stores the metadata information such as the mapping between each key and the shard that it belongs to, the latest shard, and the maximum size per shard. __init__ ( size_per_shard = None , logger = None ) Description: To initialize a sharded proto table storage with size_per_shard for each shard. Arguments: size_per_shard: the size per shard. This has to be set if the sharded proto table is newly created, and can be set None if the table already exists. logger: please see the storage_base definition. read ( params ) Description: Read from the storage. Arguments: params: the read parameters. Explanation: proto table is a key-value storage, and therefore the params need to contain field of keys , which is a list of keys in the storage. Return: a dictionary of key-values, where keys might be a subset of the input keys in the params for which the key exists in the sharded proto table storage. write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: The data needs to be a dictionary of key, value with key being a string and value being a protobuf message of any user defined types. The params can contain overwrite with its value a boolean indicating whether to overwrite the value if the key already exists in the proto table. Partitioner Storage \u00b6 Note The base class implementation of partitioners is in partitioner_base.py , it uses an underlying tree structure defined in tree_base.py . In PSLX, five types of partitioners are supported: 1. PartitionerStorageType.MINUTELY : the underlying directory will be format of 2020/03/01/00/59/ . 2. PartitionerStorageType.HOURLY : the underlying directory will be format of 2020/03/01/00/ . 3. PartitionerStorageType.DAILY : the underlying directory will be format of 2020/03/01/ . 4. PartitionerStorageType.MONTHLY : the underlying directory will be format of 2020/03/ . 5. PartitionerStorageType.YEARLY the underlying directory will be format of 2020/ . All the partitioners share with the following functions. The choice of partition type would depend on the data size. It is recommended that if the data size is huge, a more fine grained storage ( PartitionerStorageType.MINUTELY ) is used, and vice versa. set_underlying_storage ( storage ) Description: Set underlying storage behind the partitioner. Arguments: storage: any storage among StorageType.DEFAULT_STORAGE , StorageType.FIXED_SIZE_STORAGE , and StorageType.PROTO_TABLE_STORAGE . set_max_capacity ( max_capacity ) Description: Set the maximum capacity of the partitioner. Arguments: max_capacity: the maximum capacity (number of file nodes) stored in the partitioner, negative meaning the partitioner will store all the file nodes. set_config ( config ) Description: Set the config for the underlying storage. Arguments: config: the config that is added to the existing config of the underlying storage. get_dir_name () Description: Get the directory name that the partitioner is initialized from. Return: the directory name. get_size () Description: Get the current size of the partitioner file tree. Return: the size of the partitioner is_empty () Description: Check whether the partitioner is empty (no files). Return: True if empty and False otherwise. get_dir_in_timestamp ( dir_name ) Description: Get the timestamp of the directory within the partitioner. Arguments: dir_name: the directory name. Return: The formatted datetime object. get_latest_dir () Description: Get latest directory in timestamp contained in the partitioner. Return: The latest directory. get_oldest_dir () Description: Get oldest directory in timestamp contained in the partitioner. Return: The oldest directory. get_previous_dir ( cur_dir ) Description: Get the previous directory with respect to the current directory. Arguments: cur_dir: the current directory Return: The previous directory if exists, otherwise None. get_next_dir ( cur_dir ) Description: Get the next directory with respect to the current directory. Arguments: cur_dir: the current directory Return: The next directory if exists, otherwise None. read ( params ) Description: Read from the latest file in the storage. Arguments: params: the read parameters. Explanation: The underlying storage might prefer a different file name stored in each partition, hence base_name is an arg in params. The default base_name is data for StorageType.DEFAULT_STORAGE and StorageType.FIXED_SIZE_STORAGE , and data.pb for StorageType.PROTO_TABLE_STORAGE . One can also set reinitialize_underlying_storage if one wants the storage to be reinitialized. The read only load data from the latest directory. read_range ( data , params ) Description: Read a range of files from the storage. Arguments: params: the read parameters. Explanation: The params must contain start_time and end_time in order for the partitioner to retrieve files with partition within the given time range. The interval is a close interval. The return from this function will be a dictionary with file name as the key and file content as the value. If the underlying storage is a proto table, the value in the output dict will be in the format of {key_1: val_1, ... ..., key_n: val_n} with all the val_i being an Any type message. write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: The underlying storage might prefer a different file name stored in each partition, hence base_name is an arg in params. The default base_name is data for StorageType.DEFAULT_STORAGE and StorageType.FIXED_SIZE_STORAGE , and data.pb for StorageType.PROTO_TABLE_STORAGE . If make_partition is in the params and it is set False, the partition will not make new partition for the new incoming data, otherwise ( make_partition unset or set True), it will make partition based on the current timestamp. If an extra timezone field is set, the partitioner will make a new partition based on the timezone. Possible timezone could be PST , EST or UTC . If not set, the default time zone is PST . Info Debug only. print_self () Description: Print the internal file tree.","title":"Data Storage"},{"location":"storage/#default-storage-documentation","text":"read ( params ) Description: Read from the storage. Arguments: params: the read parameters. Explanation: The underlying file needs to be a text file, and the default storage supports read/write from both top and down (see the definition of ReadRuleType and WriteRuleType in schema ), and can be set by function set_config(config) by overwriting string read_rule_type and write_rule_type with the correct enum. The default values for them are ReadRuleType.READ_FROM_BEGINNING and WriteRuleType.WRITE_FROM_END . The params in the read(params) function supports the number of lines to read from the file. The way to set this field is to pass num_line to the param (a dictionary). If the num_line exceeds the total number of lines in the file, an error will be raised. After reading the underlying file, the file handler will move accordingly. For example, after reading one line from the file, the second time the storage will start by reading the second line of the file. The can be reset by calling start_from_first_line() . write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: If the data is a string, it will be written to the underlying file from top or bottom depending on the value of write_rule_type . If the data is a list, it will be joined with delimter set in the params. If key delimiter is not present in params, comma will be used by default. start_from_first_line () Description: Reset the reader to read from the first line (from top or bottom).","title":"Default Storage Documentation"},{"location":"storage/#fixed-size-storage-documentation","text":"__init__ ( logger = None , fixed_size =- 1 ) Description: Overrides the default constructor. Arguments: logger: the logging tool (see tool ) for this storage. Default value is None. fixed_size: the maximum data size that this storage will hold in memory, negative meaning the maximum size is infinity. read ( params ) Description: Read from the storage. Arguments: params: the read parameters. Explanation: Like the default storage, the underlying file needs to be a text file. The difference between fixed size storage and default storage is that fixed size storage will hold data in memory (while default storage will by default read the underlying file). The params supported here are num_line and force_load . num_line indicates the number of lines to read. If force_load if False and the num_lines exceeds the internal maximum size, error will be raised. If force_load is true, the storage will search for the file. Due to the nature of the fixed size storage, it could be used a buffer storage between disk and application. write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: Same as the writer implementation of default storage.","title":"Fixed Size Storage Documentation"},{"location":"storage/#proto-table-storage","text":"read ( params ) Description: Read from the storage. Arguments: params: the read parameters. Explanation: proto table is a key-value storage, and therefore the params need to contain field of key . The value of the proto table is a proto message, and if the field of message_type is provided, the reader will correctly deserialize to the desired protobuf. Otherwise it will only an Any type message. Under PSLX convention, the underlying file name needs to end with .pb . read_all () Description: Read all the data. Return: the key, value dictionary of the table, with value being the Any proto format. write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: The data needs to be a dictionary of key, value with key being a string and value being a protobuf message of any user defined types. The params can contain overwrite with its value a boolean indicating whether to overwrite the value if the key already exists in the proto table. delete ( key ) Description: Delete key and the corresponding entry from the proto table. Arguments: key: the key of the entry to be deleted. delete_all () Description: Delete all the contents from the proto table.","title":"Proto Table Storage"},{"location":"storage/#sharded-proto-table-storage","text":"Sharded proto table storage will shard the data into different proto tables, denoted by data@SHARD.pb , where the SHARD is an integer that starts from 0 . In addition to these tables, there also exists a index_map.pb protobuf that stores the metadata information such as the mapping between each key and the shard that it belongs to, the latest shard, and the maximum size per shard. __init__ ( size_per_shard = None , logger = None ) Description: To initialize a sharded proto table storage with size_per_shard for each shard. Arguments: size_per_shard: the size per shard. This has to be set if the sharded proto table is newly created, and can be set None if the table already exists. logger: please see the storage_base definition. read ( params ) Description: Read from the storage. Arguments: params: the read parameters. Explanation: proto table is a key-value storage, and therefore the params need to contain field of keys , which is a list of keys in the storage. Return: a dictionary of key-values, where keys might be a subset of the input keys in the params for which the key exists in the sharded proto table storage. write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: The data needs to be a dictionary of key, value with key being a string and value being a protobuf message of any user defined types. The params can contain overwrite with its value a boolean indicating whether to overwrite the value if the key already exists in the proto table.","title":"Sharded Proto Table Storage"},{"location":"storage/#partitioner-storage","text":"Note The base class implementation of partitioners is in partitioner_base.py , it uses an underlying tree structure defined in tree_base.py . In PSLX, five types of partitioners are supported: 1. PartitionerStorageType.MINUTELY : the underlying directory will be format of 2020/03/01/00/59/ . 2. PartitionerStorageType.HOURLY : the underlying directory will be format of 2020/03/01/00/ . 3. PartitionerStorageType.DAILY : the underlying directory will be format of 2020/03/01/ . 4. PartitionerStorageType.MONTHLY : the underlying directory will be format of 2020/03/ . 5. PartitionerStorageType.YEARLY the underlying directory will be format of 2020/ . All the partitioners share with the following functions. The choice of partition type would depend on the data size. It is recommended that if the data size is huge, a more fine grained storage ( PartitionerStorageType.MINUTELY ) is used, and vice versa. set_underlying_storage ( storage ) Description: Set underlying storage behind the partitioner. Arguments: storage: any storage among StorageType.DEFAULT_STORAGE , StorageType.FIXED_SIZE_STORAGE , and StorageType.PROTO_TABLE_STORAGE . set_max_capacity ( max_capacity ) Description: Set the maximum capacity of the partitioner. Arguments: max_capacity: the maximum capacity (number of file nodes) stored in the partitioner, negative meaning the partitioner will store all the file nodes. set_config ( config ) Description: Set the config for the underlying storage. Arguments: config: the config that is added to the existing config of the underlying storage. get_dir_name () Description: Get the directory name that the partitioner is initialized from. Return: the directory name. get_size () Description: Get the current size of the partitioner file tree. Return: the size of the partitioner is_empty () Description: Check whether the partitioner is empty (no files). Return: True if empty and False otherwise. get_dir_in_timestamp ( dir_name ) Description: Get the timestamp of the directory within the partitioner. Arguments: dir_name: the directory name. Return: The formatted datetime object. get_latest_dir () Description: Get latest directory in timestamp contained in the partitioner. Return: The latest directory. get_oldest_dir () Description: Get oldest directory in timestamp contained in the partitioner. Return: The oldest directory. get_previous_dir ( cur_dir ) Description: Get the previous directory with respect to the current directory. Arguments: cur_dir: the current directory Return: The previous directory if exists, otherwise None. get_next_dir ( cur_dir ) Description: Get the next directory with respect to the current directory. Arguments: cur_dir: the current directory Return: The next directory if exists, otherwise None. read ( params ) Description: Read from the latest file in the storage. Arguments: params: the read parameters. Explanation: The underlying storage might prefer a different file name stored in each partition, hence base_name is an arg in params. The default base_name is data for StorageType.DEFAULT_STORAGE and StorageType.FIXED_SIZE_STORAGE , and data.pb for StorageType.PROTO_TABLE_STORAGE . One can also set reinitialize_underlying_storage if one wants the storage to be reinitialized. The read only load data from the latest directory. read_range ( data , params ) Description: Read a range of files from the storage. Arguments: params: the read parameters. Explanation: The params must contain start_time and end_time in order for the partitioner to retrieve files with partition within the given time range. The interval is a close interval. The return from this function will be a dictionary with file name as the key and file content as the value. If the underlying storage is a proto table, the value in the output dict will be in the format of {key_1: val_1, ... ..., key_n: val_n} with all the val_i being an Any type message. write ( data , params ) Description: Write to the storage. Arguments: data: the write parameters. Explanation: The underlying storage might prefer a different file name stored in each partition, hence base_name is an arg in params. The default base_name is data for StorageType.DEFAULT_STORAGE and StorageType.FIXED_SIZE_STORAGE , and data.pb for StorageType.PROTO_TABLE_STORAGE . If make_partition is in the params and it is set False, the partition will not make new partition for the new incoming data, otherwise ( make_partition unset or set True), it will make partition based on the current timestamp. If an extra timezone field is set, the partitioner will make a new partition based on the timezone. Possible timezone could be PST , EST or UTC . If not set, the default time zone is PST . Info Debug only. print_self () Description: Print the internal file tree.","title":"Partitioner Storage"},{"location":"tool/","text":"PSLX provides a set of tools to assist development, and they include Logging tool for ttl-ed logging. File locker to ensure the atomic io of files. LRU cache for caching. SQL tool for connecting to SQL database and executing queries. Mongodb tool for connection to mongodb. Fetcher tool to fetch partitioned ProtoTable (whose values are of the same proto message type and keys are timestamps). Watcher tool to fetch partitioned ProtoTable (whose values are of the same proto message type and keys are timestamps). Registry tool to be used as decorators to register functions. Documentation for Logging Tool \u00b6 To create a logging tool instance, please use __init__ ( name , date , ttl ) Arguments: name: the name of the logger instance. date: the date that this logger is created, default to the current time. ttl: the ttl policy, default to be -1. The logger supports multiple level logging, and one can call these by using the following functions info(string, publish) : for level = info. warning(string, publish) : for level = warning. debug(string, publish) : for level = debug. error(string, publish) : for level = error. The logger output will be the format of [initial_letter_of_logger_level logger_name file_name:line_number timestamp]: the logged information Here one can send the log to backend realtime logging by setting the publisher with set_publisher(publisher) function and publish=True in the above function. The default value for publish is False . Documentation for File Locker \u00b6 FileLocker tool is used in combination within a with sentence, for instance for reading a file: with FileLocker ( protected_file_path = file_name , read_mode = True ): with open ( file_name , 'r' ) as infile : ... ... and for writing data to a file: with FileLocker ( protected_file_path = file_name , read_mode = False ): with open ( file_name , 'w' ) as outfile : ... ... FileLocker will raise error if the protected_file_path does not exist if read_mode is True. Documentation for LRU Caching \u00b6 LRU caching supports the following methods: __init__ ( max_capacity ) Description: create a LRU cache with capacity equal max_capacity. Arguments: max_capacity: the maximum capacity of the LRU cache. get ( key ) Description: get the value stored in the key in the LRU cache. Arguments: key: the key of the value. If key is not found, the function will return None. Return: the value corresponding to the key. set ( key , value ) Description: set the key value pair to the cache. Arguments: key: the key of the pair to be inserted. value: the value of the pair to be inserted. Documentation for SQL Tool \u00b6 SQL tool supports the following methods: __init__ () Description: create an instance of the SQL tool. connect_to_database ( credential , database ) Description: connect to the database name with credential. Arguments: credential: the credential used to connect to the database. database: the name of the database that is going to be connected to. execute_query_str ( query_str , modification ) Description: execute the query string. Arguments: query_str: the string of the query to be executed. modification: boolean indicating whether the query will modify the database. execute_query_file ( query_file , modification ) Description: execute the query contained in a file. Arguments: query_file: the file that contains the query. modification: boolean indicating whether the query will modify the database. Note that before executing any queries, please connect to a database first. Documentation for Mongodb Tool \u00b6 Mongodb tool supports the following methods: __init__ () Description: create an instance of the Mongodb tool. connect_to_database ( credential ) Description: connect to mongodb with credential. Arguments: credential: the credential used to the connect to mongodb. list_databases () Description: list all the database names. list_collections ( database_name ) Description: list the collection names in the database. Arguments: database_name: the name of the database. get_database ( database_name ) Decription: get the database with database name. Arguments: database_name: the name of the database. get_collection ( database_name , collection_name ) Decription: get the collection with collection name in a database. Arguments: database_name: the name of the database. collection_name: the name of the collection. Documentation for Fetcher Tool \u00b6 Tool to fetch the latest, oldest data or data within a time range. This includes implementation of LocalPartitionerFetcher where the partitioner data is local or a RemotePartitionerFetcher what fetches the data from a remote server through rpc. To initialize LocalPartitionerFetcher __init__ ( partitioner , logger ) Description: initialize a LocalPartitionerFetcher Arguments: partitioner: the partitioner object to fetch. It needs to have underlying storage of a ProtoTableStorage . logger: the logger for the fetcher. To initialize RemotePartitionerFetcher __init__ ( partitioner , server_url , logger , root_certificate ) Description: initialize a RemotePartitionerFetcher Arguments: partitioner_dir: the remote partitioner directory to fetch. It needs to have underlying storage of a ProtoTableStorage . server_url: the remote server url. logger: the logger for the fetcher. root_certificate: the root_certificate for authentication. Both LocalPartitionerFetcher and RemotePartitionerFetcher have the following implementations: fetch_latest () Description: fetch the latest data entry, sorted by the key. fetch_oldest () Description: fetch the oldest data entry, sorted by the key. fetch_range ( start_time , end_time ) Description: fetch the data whose key is within the range. Documentation for Watcher Tool \u00b6 Tool to monitor a specific latest key. This includes implementation of LocalPartitionerWatcher where the partitioner data is local or a RemotePartitionerWatcher what watches the data from a remote server through rpc. To initialize LocalPartitionerWatcher __init__ ( partitioner , logger , delay , timeout ) Description: initialize a LocalPartitionerWatcher Arguments: partitioner: the partitioner object to watch. It needs to have underlying storage of a ProtoTableStorage . logger: the logger for the watcher. delay: the seconds between each trial. timeout: the seconds of watch timeout. To initialize RemotePartitionerWatcher __init__ ( partitioner , server_url , logger , root_certificate , delay , timeout ) Description: initialize a RemotePartitionerWatcher Arguments: partitioner_dir: the remote partitioner directory to watch. It needs to have underlying storage of a ProtoTableStorage . server_url: the remote server url. logger: the logger for the fetcher. root_certificate: the root_certificate for authentication. delay: the seconds between each trial. timeout: the seconds of watch timeout. Both LocalPartitionerWatcher and RemotePartitionerWatcher have the following implementations: watch_key ( key ) Description: watch for the appearance of the given key in the latest partition. Arguments: key: the key to watch. Documentation for Registry Tool \u00b6 The registry tool can be used as a function decorator to store functions in a dictionary. The following code shows the simple usage of this tool: from pslx.tool.registry_tool import RegistryTool registry = RegistryTool () @registry . register ( \"example_func\" ) def example_func (): ... ...","title":"Tools"},{"location":"tool/#documentation-for-logging-tool","text":"To create a logging tool instance, please use __init__ ( name , date , ttl ) Arguments: name: the name of the logger instance. date: the date that this logger is created, default to the current time. ttl: the ttl policy, default to be -1. The logger supports multiple level logging, and one can call these by using the following functions info(string, publish) : for level = info. warning(string, publish) : for level = warning. debug(string, publish) : for level = debug. error(string, publish) : for level = error. The logger output will be the format of [initial_letter_of_logger_level logger_name file_name:line_number timestamp]: the logged information Here one can send the log to backend realtime logging by setting the publisher with set_publisher(publisher) function and publish=True in the above function. The default value for publish is False .","title":"Documentation for Logging Tool"},{"location":"tool/#documentation-for-file-locker","text":"FileLocker tool is used in combination within a with sentence, for instance for reading a file: with FileLocker ( protected_file_path = file_name , read_mode = True ): with open ( file_name , 'r' ) as infile : ... ... and for writing data to a file: with FileLocker ( protected_file_path = file_name , read_mode = False ): with open ( file_name , 'w' ) as outfile : ... ... FileLocker will raise error if the protected_file_path does not exist if read_mode is True.","title":"Documentation for File Locker"},{"location":"tool/#documentation-for-lru-caching","text":"LRU caching supports the following methods: __init__ ( max_capacity ) Description: create a LRU cache with capacity equal max_capacity. Arguments: max_capacity: the maximum capacity of the LRU cache. get ( key ) Description: get the value stored in the key in the LRU cache. Arguments: key: the key of the value. If key is not found, the function will return None. Return: the value corresponding to the key. set ( key , value ) Description: set the key value pair to the cache. Arguments: key: the key of the pair to be inserted. value: the value of the pair to be inserted.","title":"Documentation for LRU Caching"},{"location":"tool/#documentation-for-sql-tool","text":"SQL tool supports the following methods: __init__ () Description: create an instance of the SQL tool. connect_to_database ( credential , database ) Description: connect to the database name with credential. Arguments: credential: the credential used to connect to the database. database: the name of the database that is going to be connected to. execute_query_str ( query_str , modification ) Description: execute the query string. Arguments: query_str: the string of the query to be executed. modification: boolean indicating whether the query will modify the database. execute_query_file ( query_file , modification ) Description: execute the query contained in a file. Arguments: query_file: the file that contains the query. modification: boolean indicating whether the query will modify the database. Note that before executing any queries, please connect to a database first.","title":"Documentation for SQL Tool"},{"location":"tool/#documentation-for-mongodb-tool","text":"Mongodb tool supports the following methods: __init__ () Description: create an instance of the Mongodb tool. connect_to_database ( credential ) Description: connect to mongodb with credential. Arguments: credential: the credential used to the connect to mongodb. list_databases () Description: list all the database names. list_collections ( database_name ) Description: list the collection names in the database. Arguments: database_name: the name of the database. get_database ( database_name ) Decription: get the database with database name. Arguments: database_name: the name of the database. get_collection ( database_name , collection_name ) Decription: get the collection with collection name in a database. Arguments: database_name: the name of the database. collection_name: the name of the collection.","title":"Documentation for Mongodb Tool"},{"location":"tool/#documentation-for-fetcher-tool","text":"Tool to fetch the latest, oldest data or data within a time range. This includes implementation of LocalPartitionerFetcher where the partitioner data is local or a RemotePartitionerFetcher what fetches the data from a remote server through rpc. To initialize LocalPartitionerFetcher __init__ ( partitioner , logger ) Description: initialize a LocalPartitionerFetcher Arguments: partitioner: the partitioner object to fetch. It needs to have underlying storage of a ProtoTableStorage . logger: the logger for the fetcher. To initialize RemotePartitionerFetcher __init__ ( partitioner , server_url , logger , root_certificate ) Description: initialize a RemotePartitionerFetcher Arguments: partitioner_dir: the remote partitioner directory to fetch. It needs to have underlying storage of a ProtoTableStorage . server_url: the remote server url. logger: the logger for the fetcher. root_certificate: the root_certificate for authentication. Both LocalPartitionerFetcher and RemotePartitionerFetcher have the following implementations: fetch_latest () Description: fetch the latest data entry, sorted by the key. fetch_oldest () Description: fetch the oldest data entry, sorted by the key. fetch_range ( start_time , end_time ) Description: fetch the data whose key is within the range.","title":"Documentation for Fetcher Tool"},{"location":"tool/#documentation-for-watcher-tool","text":"Tool to monitor a specific latest key. This includes implementation of LocalPartitionerWatcher where the partitioner data is local or a RemotePartitionerWatcher what watches the data from a remote server through rpc. To initialize LocalPartitionerWatcher __init__ ( partitioner , logger , delay , timeout ) Description: initialize a LocalPartitionerWatcher Arguments: partitioner: the partitioner object to watch. It needs to have underlying storage of a ProtoTableStorage . logger: the logger for the watcher. delay: the seconds between each trial. timeout: the seconds of watch timeout. To initialize RemotePartitionerWatcher __init__ ( partitioner , server_url , logger , root_certificate , delay , timeout ) Description: initialize a RemotePartitionerWatcher Arguments: partitioner_dir: the remote partitioner directory to watch. It needs to have underlying storage of a ProtoTableStorage . server_url: the remote server url. logger: the logger for the fetcher. root_certificate: the root_certificate for authentication. delay: the seconds between each trial. timeout: the seconds of watch timeout. Both LocalPartitionerWatcher and RemotePartitionerWatcher have the following implementations: watch_key ( key ) Description: watch for the appearance of the given key in the latest partition. Arguments: key: the key to watch.","title":"Documentation for Watcher Tool"},{"location":"tool/#documentation-for-registry-tool","text":"The registry tool can be used as a function decorator to store functions in a dictionary. The following code shows the simple usage of this tool: from pslx.tool.registry_tool import RegistryTool registry = RegistryTool () @registry . register ( \"example_func\" ) def example_func (): ... ...","title":"Documentation for Registry Tool"},{"location":"ttl_cleaner/","text":"PSLX stores the temporary data including: Snapshots of operators and containers. Log files. RPC requests and responses. and these temporary data are stored in the path defined py the environment variable PSLX_DATABASE , whose default value is database/ . To make sure these temporary files do not occupy too much disk, PSLX designs a retention policy (ttl) for these files by putting everything ttl=TIME . We also run a separate ttl clearer to clean the files, and the criteria is that, if the last modified time for a file is already TIME away from the current time, the ttl cleaner will delete it. The TIME in the ttl defined path could be an integer or a string. If this value is an integer, then the unit by default is day. For string value time, one can specify the unit of m as minute, h as hour and d as day. For example, a ttl=1m folder will guarantee that all the files in the folder that are not updated within the past 1 minute will be deleted if the ttl cleaner sees it. The current ttl cleaner implementation exists in ttl_cleaner , there is also an example about how to run the ttl cleaner in the example folder. from pslx.micro_service.ttl_cleaner.ttl_cleaner import TTLCleaner if __name__ == \"__main__\" : ttl_cleaner = TTLCleaner () ttl_cleaner . set_schedule ( hour = '*' , minute = '*/10' ) ttl_cleaner . set_max_instances ( max_instances = 5 ) ttl_cleaner . start () Fundamentally, ttl cleaner is designed as an CronBatchContainer with container name PSLX_TTL_CLEANER_OPERATOR , and like regular cron jobs, you can specify the hour and minute that the ttl cleaner runs. The above example shows the ttl cleaner runs every hour at every 10 min. Also please make sure your PSLX_DATABASE is consistent across your application, because ttl cleaner looks at files under this folder by default. ttl cleaner also allows user to specify other folders in addition to PSLX_DATABASE . The way to do that is through the following function: watch_dir ( dir_name ) Description: Add other directory to watch. Arguments: dir_name: the directory that wants ttl cleaner to watch and clean.","title":"TTL Cleaner"},{"location":"util/","text":"PSLX provides a comprehensive set of utility functions including file_util.py : utilities related to files and directories. proto_util.py : utilities for protobuf. timezone_util.py : utilities for timestamp related. dummy_util.py : a wrapper of dummy tools. yaml.util.py : utility for yaml files. common_util.py : utility for creating credentials. env_util.py : utility for PSLX built-in environment variables. decorator_util.py : utility for commonly used decorators. Info All the below functions are labeled as classmethods , and can be called without initiating any instance of the class. Documentation of File Utilities \u00b6 The following are functions related to files and directories. base_name ( file_name ) Description: get the base name of a file. Arguments: file_name: the name of the file. dir_name ( file_name ) Description: get the directory of a file. Arguments: file_name: the name of the file. does_file_exist ( file_name ) Description: check whether a file exist or not. Arguments: file_name: the name of the file. does_dir_exist ( dir_name ) Description: check whether a directory exist or not. Arguments: dir_name: the name of the directory. is_file_empty ( file_name ) Description: check whether a file is empty. Arguments: file_name: the name of the file. is_dir_empty ( dir_name ) Description: check whether a file is empty. Arguments: dir_name: the name of the directory. create_file_if_not_exist ( file_name ) Description: create a file if the file does not exist Arguments: file_name: the name of the file. create_dir_if_not_exist ( dir_name ) Description: create a directory if the file does not exist Arguments: dir_name: the name of the directory. die_if_file_not_exist ( file_name ) Description: raise error if a file does not exist. Arguments: file_name: the name of the file. die_if_dir_not_exist ( dir_name ) Description: raise error if a directory does not exist. Arguments: dir_name: the name of the directory. is_file ( path_name ) Description: check whether the given path is a file. Arguments: path_name: the name of the path. is_dir ( path_name ) Description: check whether the given path is a directory. Arguments: path_name: the name of the path. normalize_file_name ( file_name ) Description: normalize a file name. Arguments: file_name: the name of the file. normalize_dir_name ( dir_name ) Description: normalize a directory name. Arguments: dir_name: the name of the directory. list_files_in_dir ( dir_name ) Description: list all files in a directory. Arguments: dir_name: the name of the directory. list_dirs_in_dir ( dir_name ) Description: list all sub directories in a directory. Arguments: dir_name: the name of the directory. list_files_in_dir_recursively ( proto_message ) Description: list all files in a directory recursively. Arguments: dir_name: the name of the directory. list_dirs_in_dir_recursively ( proto_message ) Description: list all sub directories in a directory recursively. Arguments: dir_name: the name of the directory. remove_file ( file_name ) Description: remove a file. Arguments: file_name: the name of the file to be removed. remove_dir_recursively ( dir_name ) Description: remove sub directories recursively from a directory. Arguments: dir_name: the name of the directory. get_file_modified_time ( file_name ) Description: get the last modified time of a file. Arguments: file_name: the name of the file. get_ttl_from_path ( path_name ) Description: get the ttl from a path. Arguments: path_name: the path, a file or directory. join_paths_to_file_with_mode ( root_dir , base_name , ttl ) Description: join root directory, base name, ttl, and the mode, to a file name. Arguments: root_dir: the root directory of the file. base_name: the base name of the file. ttl: the ttl of the file. join_paths_to_dir_with_mode ( root_dir , base_name , ttl ) Description: join root directory, base name, ttl, and the mode, to a directory name. Arguments: root_dir: the root directory of the directory. base_name: the base name of the directory. ttl: the ttl of the file. join_paths_to_file ( root_dir , base_name , ttl ) Description: join root directory, base name, and ttl, to a file name. Arguments: root_dir: the root directory of the file. base_name: the base name of the file. ttl: the ttl of the file. join_paths_to_dir ( root_dir , base_name , ttl ) Description: join root directory, base name, and ttl, to a directory name. Arguments: root_dir: the root directory of the directory. base_name: the base name of the directory. ttl: the ttl of the file. get_file_names_from_pattern ( pattern ) Description: get all the files with the pattern. Arguments: pattern: the pattern of the files. get_mode ( path_name ) Description: get the mode from a path. Arguments: path_name: the path to a file or directory. write_proto_to_file ( proto , file_name ) Description: write a proto message to a file. Arguments: proto: the proto message to be written. file_name: the output file name. read_proto_from_file ( proto_type , file_name ) Description: read a proto message with given type from a file. Arguments: proto_type: the type of the proto message. file_name: the name of the file containing the message. create_container_snapshot_pattern ( container_name , container_class , container_ttl ) Description: get the container snapshot pattern. Arguments: container_name: the name of the container. container_class: the class of the container. container_ttl: the ttl of the container. create_operator_snapshot_pattern ( container_name , operator_name , container_class , container_ttl ) Description: get the operator snapshot pattern. Arguments: container_name: the name of the container. operator_name: the name of the operator. container_class: the class of the container. container_ttl: the ttl of the container. get_file_size ( file_name ) Description: get a string representation of the size of the file. Arguments: file_name: the name of the file. Documentation of Protobuf Utilities \u00b6 THe following functions are related to protobuf. check_valid_enum ( enum_type , value ) Description: check whether a enum type value is valid. Arguments: enum_type: the enum type of the value. value: the value to be checked. get_name_by_value ( enum_type , value ) Description: get the name of the enum type value. Arguments: enum_type: the enum type of the value. value: the value to be examined. get_value_by_name ( enum_type , name ) Description: get the value of the enum type name. Arguments: enum_type: the enum type of the name. name: the name to be examined. get_name_by_value_and_enum_name ( enum_type_str , value ) Description: get the name of the enum type value when the enum type is represented by a string. Arguments: enum_type_str: the string representation of the enum type. value: the value to be examined. get_value_by_name_and_enum_name ( enum_type_str , name ) Description: get the value of the enum type value when the enum type is represented by a string. Arguments: enum_type_str: the string representation of the enum type. name: the name to be examined. message_to_json ( proto_message ) Description: convert a proto message to json. Arguments: proto_message: the proto message to be converted. message_to_string ( proto_message ) Description: convert a proto message to plain string. Arguments: proto_message: the proto message to be converted. message_to_text ( proto_message ) Description: convert a proto message to text. Arguments: proto_message: the proto message to be converted. json_to_message ( message_type , json_str ) Description: convert a json to a proto message in a given type. Arguments: message_type: the type of the output proto message. json_str: the json to be converted. string_to_message ( message_type , string ) Description: convert a plain string to a proto message in a given type. Arguments: message_type: the type of the output proto message. string: the plain string to be converted. text_to_message ( message_type , text_str ) Description: convert a text to a proto message in a given type. Arguments: message_type: the type of the output proto message. text_str: the text to be converted. message_to_any ( message ) Description: convert a proto message to Any type. Arguments: message: the message to be converted. any_to_message ( message_type , any_message ) Description: convert an Any type message to a given type message. Arguments: message_type: the type of the output proto message. any_message: the Any message to be converted. infer_message_type_from_str ( message_type_str , modules ) Description: infer the message type represented by a string. Arguments: message_type_str: the string representation of the message type. modules: the possible proto modules to search. infer_str_from_message_type ( message_type ) Description: infer the string representation of a given message type. Arguments: message_type: the proto message type. Documentation of Timezone Utilities \u00b6 The timestamp related utility class includes the following methods: utc_to_pst ( utc_time ) Description: convert a utc datetime object to a pst datetime object. Arguments: utc_time: the utc datetime object. utc_to_est ( utc_time ) Description: convert a utc datetime object to an est datetime object. Arguments: utc_time: the utc datetime object. pst_to_est ( western_time ) Description: convert a pst datetime object to an est datetime object. Arguments: western_time: the pst datetime object. est_to_pst ( eastern_time ) Description: convert an est datetime object to a pst datetime object. Arguments: eastern_time: the est datetime object. cur_time_in_local () Description: the current time in local timezone. cur_time_in_utc () Description: the current time in utc timezone. cur_time_in_pst () Description: the current time in pst timezone. cur_time_in_est () Description: the current time in est timezone. cur_time_in_est ( time_str ) Description: convert a string representation of datetime object to datetime object. Arguments: time_str: the string representation of datetime object. Documentation of Dummy Utilities \u00b6 The DummyUtil class is a wrapper of a few dummy entities that can be used. Here dummy means that any method inside them are dummy function calls. dummy_logging () Description: return a dummy logger that acts as a placeholder but does not log. Note The above operators are very helpful for containers that have only one meaningful operator. Please check batch_container_example and streaming_container_example . dummy_streaming_operator ( operator_name ) Description: return a dummy streaming operator tha does not execute anything. Arguments: operator_name: the name of the operator. dummy_batch_operator ( operator_name ) Description: return a dummy batch operator tha does not execute anything. Arguments: operator_name: the name of the operator. Documentation of Yaml Utilities \u00b6 The following method is provided yaml_to_dict ( file_name ) Description: read a yaml file to dictionary. Arguments: file_name: the yaml file name. Return: the dictionary representation of the content in the yaml file. Documentation of Common Utilities \u00b6 The following methods are provided make_email_credentials ( email_addr , password , email_server , email_server_port ) Description: make a credential for email RPC server. Arguments: email_addr: the email address. password: the password to the email. email_server: the smtp server of the email, default to be \"smtp.gmail.com\". email_server_port: the port for the email server, default to be 25. make_sql_server_credentials ( sql_host_ip , sql_port , user_name , password ) Description: make a credential for SQL server. Arguments: sql_host_ip: the host server ip for the SQL database. sql_port: the port on the server ip for the SQL database. user_name: the user name to login the SQL database. password: the password to login the SQL database. make_frontend_config ( yaml_path ) Description: make a credential for PSLX frontend. Arguments: yaml_path: the path to the config yaml file. Please check section frontend . Documentation of Environment Utilities \u00b6 The environment variables inside PSLX are defined in a dictionary: PSLX_ENV_TO_DEFAULT_MAP = { 'PSLX_INTERNAL_TTL' : 7 , 'PSLX_INTERNAL_CACHE' : 100 , 'PSLX_TEST' : False , 'PSLX_LOG' : False , 'PSLX_DATABASE' : 'database/' , 'PSLX_GRPC_MAX_MESSAGE_LENGTH' : 512 * 1024 * 1024 , # 512MB, 'PSLX_GRPC_TIMEOUT' : 1 , # 1 second 'PSLX_QUEUE_TIMEOUT' : 10 , # 10 seconds 'PSLX_FRONTEND_CONFIG_PROTO_PATH' : '' , \"PSLX_RPC_FLUSH_RATE\" : 1 , 'PSLX_RPC_PASSWORD' : 'admin' , } , and this utility allows user to access the above variables by get_pslx_env_variable ( var , fallback_value ) Arguments: var: the environment variable name. fallback_value: the fallback value if the var does not have a default value defined by PSLX. Documentation of Decorator Utilities \u00b6 The decorator utility currently supports adding a run condition (a callback function returns True for False ) to a function. For example: from pslx.util.decorator_util import DecoratorUtil from pslx.util.timezone_util import TimezoneUtil def condition_func ( weekday ): cur_time = TimezoneUtil . cur_time_in_pst () if cur_time . weekday () == weekday : return True else : return False @DecoratorUtil . run_on_condition ( condition_func = condition_func , weekday = 5 ) def test_func ( test_string ): return test_string if __name__ == \"__main__\" : print ( test_func ( \"test here\" )) Then this function will only print out the string on Saturday. The decorator also supports adding ranges to hours and minutes. Decorator utility also supports timeout inside thread or in the main thread. The usage is from pslx.util.decorator_util import DecoratorUtil @DecoratorUtil . default_timeout ( time_out = 6 ) def f (): time . sleep ( 5 ) @DecoratorUtil . thread_safe_timeout ( time_out = 2 ) def g (): time . sleep ( 5 ) if __name__ == \"__main__\" : f () g () In decorator utils, function rate limit decorated is implemented and can be used as from pslx.util.decorator_util import DecoratorUtil @DecoratorUtil . rate_limiter ( interval = 1 ) def f (): print ( \"hello world\" ) if __name__ == \"__main__\" : for _ in range ( 10 ): f () # will be rated limited to 1s per print The retry decorator can be used in the following way from pslx.util.decorator_util import DecoratorUtil if __name__ == \"__main__\" : hit = [ 0 ] @DecoratorUtil . retry ( retry_on_exception = ValueError , num_retry = 1 ) def test ( val ): for _ in range ( val ): hit [ 0 ] += 1 if hit [ 0 ] == 5 : return 10 else : raise ValueError print ( test ( 3 ))","title":"Utilities"},{"location":"util/#documentation-of-file-utilities","text":"The following are functions related to files and directories. base_name ( file_name ) Description: get the base name of a file. Arguments: file_name: the name of the file. dir_name ( file_name ) Description: get the directory of a file. Arguments: file_name: the name of the file. does_file_exist ( file_name ) Description: check whether a file exist or not. Arguments: file_name: the name of the file. does_dir_exist ( dir_name ) Description: check whether a directory exist or not. Arguments: dir_name: the name of the directory. is_file_empty ( file_name ) Description: check whether a file is empty. Arguments: file_name: the name of the file. is_dir_empty ( dir_name ) Description: check whether a file is empty. Arguments: dir_name: the name of the directory. create_file_if_not_exist ( file_name ) Description: create a file if the file does not exist Arguments: file_name: the name of the file. create_dir_if_not_exist ( dir_name ) Description: create a directory if the file does not exist Arguments: dir_name: the name of the directory. die_if_file_not_exist ( file_name ) Description: raise error if a file does not exist. Arguments: file_name: the name of the file. die_if_dir_not_exist ( dir_name ) Description: raise error if a directory does not exist. Arguments: dir_name: the name of the directory. is_file ( path_name ) Description: check whether the given path is a file. Arguments: path_name: the name of the path. is_dir ( path_name ) Description: check whether the given path is a directory. Arguments: path_name: the name of the path. normalize_file_name ( file_name ) Description: normalize a file name. Arguments: file_name: the name of the file. normalize_dir_name ( dir_name ) Description: normalize a directory name. Arguments: dir_name: the name of the directory. list_files_in_dir ( dir_name ) Description: list all files in a directory. Arguments: dir_name: the name of the directory. list_dirs_in_dir ( dir_name ) Description: list all sub directories in a directory. Arguments: dir_name: the name of the directory. list_files_in_dir_recursively ( proto_message ) Description: list all files in a directory recursively. Arguments: dir_name: the name of the directory. list_dirs_in_dir_recursively ( proto_message ) Description: list all sub directories in a directory recursively. Arguments: dir_name: the name of the directory. remove_file ( file_name ) Description: remove a file. Arguments: file_name: the name of the file to be removed. remove_dir_recursively ( dir_name ) Description: remove sub directories recursively from a directory. Arguments: dir_name: the name of the directory. get_file_modified_time ( file_name ) Description: get the last modified time of a file. Arguments: file_name: the name of the file. get_ttl_from_path ( path_name ) Description: get the ttl from a path. Arguments: path_name: the path, a file or directory. join_paths_to_file_with_mode ( root_dir , base_name , ttl ) Description: join root directory, base name, ttl, and the mode, to a file name. Arguments: root_dir: the root directory of the file. base_name: the base name of the file. ttl: the ttl of the file. join_paths_to_dir_with_mode ( root_dir , base_name , ttl ) Description: join root directory, base name, ttl, and the mode, to a directory name. Arguments: root_dir: the root directory of the directory. base_name: the base name of the directory. ttl: the ttl of the file. join_paths_to_file ( root_dir , base_name , ttl ) Description: join root directory, base name, and ttl, to a file name. Arguments: root_dir: the root directory of the file. base_name: the base name of the file. ttl: the ttl of the file. join_paths_to_dir ( root_dir , base_name , ttl ) Description: join root directory, base name, and ttl, to a directory name. Arguments: root_dir: the root directory of the directory. base_name: the base name of the directory. ttl: the ttl of the file. get_file_names_from_pattern ( pattern ) Description: get all the files with the pattern. Arguments: pattern: the pattern of the files. get_mode ( path_name ) Description: get the mode from a path. Arguments: path_name: the path to a file or directory. write_proto_to_file ( proto , file_name ) Description: write a proto message to a file. Arguments: proto: the proto message to be written. file_name: the output file name. read_proto_from_file ( proto_type , file_name ) Description: read a proto message with given type from a file. Arguments: proto_type: the type of the proto message. file_name: the name of the file containing the message. create_container_snapshot_pattern ( container_name , container_class , container_ttl ) Description: get the container snapshot pattern. Arguments: container_name: the name of the container. container_class: the class of the container. container_ttl: the ttl of the container. create_operator_snapshot_pattern ( container_name , operator_name , container_class , container_ttl ) Description: get the operator snapshot pattern. Arguments: container_name: the name of the container. operator_name: the name of the operator. container_class: the class of the container. container_ttl: the ttl of the container. get_file_size ( file_name ) Description: get a string representation of the size of the file. Arguments: file_name: the name of the file.","title":"Documentation of File Utilities"},{"location":"util/#documentation-of-protobuf-utilities","text":"THe following functions are related to protobuf. check_valid_enum ( enum_type , value ) Description: check whether a enum type value is valid. Arguments: enum_type: the enum type of the value. value: the value to be checked. get_name_by_value ( enum_type , value ) Description: get the name of the enum type value. Arguments: enum_type: the enum type of the value. value: the value to be examined. get_value_by_name ( enum_type , name ) Description: get the value of the enum type name. Arguments: enum_type: the enum type of the name. name: the name to be examined. get_name_by_value_and_enum_name ( enum_type_str , value ) Description: get the name of the enum type value when the enum type is represented by a string. Arguments: enum_type_str: the string representation of the enum type. value: the value to be examined. get_value_by_name_and_enum_name ( enum_type_str , name ) Description: get the value of the enum type value when the enum type is represented by a string. Arguments: enum_type_str: the string representation of the enum type. name: the name to be examined. message_to_json ( proto_message ) Description: convert a proto message to json. Arguments: proto_message: the proto message to be converted. message_to_string ( proto_message ) Description: convert a proto message to plain string. Arguments: proto_message: the proto message to be converted. message_to_text ( proto_message ) Description: convert a proto message to text. Arguments: proto_message: the proto message to be converted. json_to_message ( message_type , json_str ) Description: convert a json to a proto message in a given type. Arguments: message_type: the type of the output proto message. json_str: the json to be converted. string_to_message ( message_type , string ) Description: convert a plain string to a proto message in a given type. Arguments: message_type: the type of the output proto message. string: the plain string to be converted. text_to_message ( message_type , text_str ) Description: convert a text to a proto message in a given type. Arguments: message_type: the type of the output proto message. text_str: the text to be converted. message_to_any ( message ) Description: convert a proto message to Any type. Arguments: message: the message to be converted. any_to_message ( message_type , any_message ) Description: convert an Any type message to a given type message. Arguments: message_type: the type of the output proto message. any_message: the Any message to be converted. infer_message_type_from_str ( message_type_str , modules ) Description: infer the message type represented by a string. Arguments: message_type_str: the string representation of the message type. modules: the possible proto modules to search. infer_str_from_message_type ( message_type ) Description: infer the string representation of a given message type. Arguments: message_type: the proto message type.","title":"Documentation of Protobuf Utilities"},{"location":"util/#documentation-of-timezone-utilities","text":"The timestamp related utility class includes the following methods: utc_to_pst ( utc_time ) Description: convert a utc datetime object to a pst datetime object. Arguments: utc_time: the utc datetime object. utc_to_est ( utc_time ) Description: convert a utc datetime object to an est datetime object. Arguments: utc_time: the utc datetime object. pst_to_est ( western_time ) Description: convert a pst datetime object to an est datetime object. Arguments: western_time: the pst datetime object. est_to_pst ( eastern_time ) Description: convert an est datetime object to a pst datetime object. Arguments: eastern_time: the est datetime object. cur_time_in_local () Description: the current time in local timezone. cur_time_in_utc () Description: the current time in utc timezone. cur_time_in_pst () Description: the current time in pst timezone. cur_time_in_est () Description: the current time in est timezone. cur_time_in_est ( time_str ) Description: convert a string representation of datetime object to datetime object. Arguments: time_str: the string representation of datetime object.","title":"Documentation of Timezone Utilities"},{"location":"util/#documentation-of-dummy-utilities","text":"The DummyUtil class is a wrapper of a few dummy entities that can be used. Here dummy means that any method inside them are dummy function calls. dummy_logging () Description: return a dummy logger that acts as a placeholder but does not log. Note The above operators are very helpful for containers that have only one meaningful operator. Please check batch_container_example and streaming_container_example . dummy_streaming_operator ( operator_name ) Description: return a dummy streaming operator tha does not execute anything. Arguments: operator_name: the name of the operator. dummy_batch_operator ( operator_name ) Description: return a dummy batch operator tha does not execute anything. Arguments: operator_name: the name of the operator.","title":"Documentation of Dummy Utilities"},{"location":"util/#documentation-of-yaml-utilities","text":"The following method is provided yaml_to_dict ( file_name ) Description: read a yaml file to dictionary. Arguments: file_name: the yaml file name. Return: the dictionary representation of the content in the yaml file.","title":"Documentation of Yaml Utilities"},{"location":"util/#documentation-of-common-utilities","text":"The following methods are provided make_email_credentials ( email_addr , password , email_server , email_server_port ) Description: make a credential for email RPC server. Arguments: email_addr: the email address. password: the password to the email. email_server: the smtp server of the email, default to be \"smtp.gmail.com\". email_server_port: the port for the email server, default to be 25. make_sql_server_credentials ( sql_host_ip , sql_port , user_name , password ) Description: make a credential for SQL server. Arguments: sql_host_ip: the host server ip for the SQL database. sql_port: the port on the server ip for the SQL database. user_name: the user name to login the SQL database. password: the password to login the SQL database. make_frontend_config ( yaml_path ) Description: make a credential for PSLX frontend. Arguments: yaml_path: the path to the config yaml file. Please check section frontend .","title":"Documentation of Common Utilities"},{"location":"util/#documentation-of-environment-utilities","text":"The environment variables inside PSLX are defined in a dictionary: PSLX_ENV_TO_DEFAULT_MAP = { 'PSLX_INTERNAL_TTL' : 7 , 'PSLX_INTERNAL_CACHE' : 100 , 'PSLX_TEST' : False , 'PSLX_LOG' : False , 'PSLX_DATABASE' : 'database/' , 'PSLX_GRPC_MAX_MESSAGE_LENGTH' : 512 * 1024 * 1024 , # 512MB, 'PSLX_GRPC_TIMEOUT' : 1 , # 1 second 'PSLX_QUEUE_TIMEOUT' : 10 , # 10 seconds 'PSLX_FRONTEND_CONFIG_PROTO_PATH' : '' , \"PSLX_RPC_FLUSH_RATE\" : 1 , 'PSLX_RPC_PASSWORD' : 'admin' , } , and this utility allows user to access the above variables by get_pslx_env_variable ( var , fallback_value ) Arguments: var: the environment variable name. fallback_value: the fallback value if the var does not have a default value defined by PSLX.","title":"Documentation of Environment Utilities"},{"location":"util/#documentation-of-decorator-utilities","text":"The decorator utility currently supports adding a run condition (a callback function returns True for False ) to a function. For example: from pslx.util.decorator_util import DecoratorUtil from pslx.util.timezone_util import TimezoneUtil def condition_func ( weekday ): cur_time = TimezoneUtil . cur_time_in_pst () if cur_time . weekday () == weekday : return True else : return False @DecoratorUtil . run_on_condition ( condition_func = condition_func , weekday = 5 ) def test_func ( test_string ): return test_string if __name__ == \"__main__\" : print ( test_func ( \"test here\" )) Then this function will only print out the string on Saturday. The decorator also supports adding ranges to hours and minutes. Decorator utility also supports timeout inside thread or in the main thread. The usage is from pslx.util.decorator_util import DecoratorUtil @DecoratorUtil . default_timeout ( time_out = 6 ) def f (): time . sleep ( 5 ) @DecoratorUtil . thread_safe_timeout ( time_out = 2 ) def g (): time . sleep ( 5 ) if __name__ == \"__main__\" : f () g () In decorator utils, function rate limit decorated is implemented and can be used as from pslx.util.decorator_util import DecoratorUtil @DecoratorUtil . rate_limiter ( interval = 1 ) def f (): print ( \"hello world\" ) if __name__ == \"__main__\" : for _ in range ( 10 ): f () # will be rated limited to 1s per print The retry decorator can be used in the following way from pslx.util.decorator_util import DecoratorUtil if __name__ == \"__main__\" : hit = [ 0 ] @DecoratorUtil . retry ( retry_on_exception = ValueError , num_retry = 1 ) def test ( val ): for _ in range ( val ): hit [ 0 ] += 1 if hit [ 0 ] == 5 : return 10 else : raise ValueError print ( test ( 3 ))","title":"Documentation of Decorator Utilities"},{"location":"websocket/","text":"PSLX implements a basic Websocket client in websocket_base.py , where the following methods are supported: __init__ ( ws_url , params , logger ) Description: Initialize a websocket client. Arguments: ws_url: the websocket URL. params: other parameters if necessary. It should be a dict. logger: the logger to record events. bind_to_op ( op ) Description: bind the websocket client to an operator. Arguments: op: the operator to be binded. In order to bind correctly, the operator needs to have a ws_msg_parser(message) function defined as one of the methods. start () Description: start to consume the websocket.","title":"Websocket"},{"location":"micro_services/email/","text":"PSLX supports sending email over RPC. The implementation of the RPC is defined in email/rpc.py , and the following class methods are supported: add_email_credentials ( credentials ) Description: add new email credentials. The credentials can be composed using a function defined in common_util.py . Arguments: credentials: the proto for email credential (see schema ) There is also example about how to launch the RPC server in example/email_example/server.py . import os from pslx.micro_service.email.rpc import EmailRPC from pslx.micro_service.rpc.generic_server import GenericServer from pslx.util.common_util import CommonUtil if __name__ == \"__main__\" : server_url = \"localhost:11443\" example_rpc = EmailRPC ( rpc_storage = None ) credentials = CommonUtil . make_email_credentials ( email_addr = 'alphahunter2019@gmail.com' , password = os . getenv ( 'PSLX_EMAIL_PWD' , '' ) ) example_rpc . add_email_credentials ( credentials = credentials ) example_server = GenericServer ( server_name = 'example' ) example_server . create_server ( max_worker = 1 , server_url = server_url ) example_server . bind_rpc ( rpc = example_rpc ) example_server . start_server () The client of the email service also has an example int example/email_example/client.py . from pslx.micro_service.email.client import EmailRPCClient if __name__ == \"__main__\" : server_url = \"localhost:11443\" email_client = EmailRPCClient ( client_name = 'example email' , server_url = server_url ) email_client . send_email ( from_email = 'alphahunter2019@gmail.com' , to_email = 'kfrancischen@gmail.com' , content = 'this is a test.' )","title":"Email"},{"location":"micro_services/frontend/","text":"PSLX supports a frontend UI for viewing proto and browsing files. The implementation is based on Flask, and is in the folder . The frontend UI is made of four parts: 1. RPC health checker: The index page will show the connection to the RPC servers, including the proto viewer RPC server, file viewer and container backend RPC server. 2. Proto Viewer: view the content in the proto. 3. File Viewer: browse files in the server. 4. Container Backend: the status of the containers and operators. On example way of starting proto viewer server is at proto_viewer_example/server.py : from pslx.micro_service.proto_viewer.rpc import ProtoViewerRPC from pslx.micro_service.rpc.generic_server import GenericServer from pslx.storage.partitioner_storage import MinutelyPartitionerStorage from pslx.util.file_util import FileUtil if __name__ == \"__main__\" : server_url = \"localhost:11444\" partitioner_dir = FileUtil . join_paths_to_dir_with_mode ( root_dir = 'database/proto_viewer/' , base_name = 'proto_viewer_example' , ttl = '1h' ) storage = MinutelyPartitionerStorage () storage . initialize_from_dir ( dir_name = partitioner_dir ) example_rpc = ProtoViewerRPC ( rpc_storage = storage ) example_server = GenericServer ( server_name = 'example' ) example_server . create_server ( max_worker = 1 , server_url = server_url ) example_server . bind_rpc ( rpc = example_rpc ) example_server . start_server () On example way of starting file viewer server is at file_viewer_example/server.py : from pslx.micro_service.file_viewer.rpc import FileViewerRPC from pslx.micro_service.rpc.generic_server import GenericServer from pslx.storage.partitioner_storage import MinutelyPartitionerStorage from pslx.util.file_util import FileUtil if __name__ == \"__main__\" : server_url = \"localhost:11445\" partitioner_dir = FileUtil . join_paths_to_dir_with_mode ( root_dir = 'database/file_viewer/' , base_name = 'file_viewer_example' , ttl = '1h' ) storage = MinutelyPartitionerStorage () storage . initialize_from_dir ( dir_name = partitioner_dir ) example_rpc = FileViewerRPC ( rpc_storage = storage ) example_server = GenericServer ( server_name = 'example' ) example_server . create_server ( max_worker = 1 , server_url = server_url ) example_server . bind_rpc ( rpc = example_rpc ) example_server . start_server () On example way of starting a container backend is at container_backend_example/server.py : from pslx.micro_service.container_backend.rpc import ContainerBackendRPC from pslx.micro_service.rpc.generic_server import GenericServer from pslx.storage.partitioner_storage import MinutelyPartitionerStorage from pslx.util.file_util import FileUtil if __name__ == \"__main__\" : server_url = \"localhost:11443\" partitioner_dir = FileUtil . join_paths_to_dir_with_mode ( root_dir = 'database/container_backend/' , base_name = 'container_backend_example' , ttl = 1 ) storage = MinutelyPartitionerStorage () storage . initialize_from_dir ( dir_name = partitioner_dir ) example_rpc = ContainerBackendRPC ( rpc_storage = storage ) example_server = GenericServer ( server_name = 'example_backend' ) example_server . create_server ( max_worker = 1 , server_url = server_url ) example_server . bind_rpc ( rpc = example_rpc ) example_server . start_server () To start the frontend, one needs to create a YAML file containing the settings. An example of it can be found at frontend_example/frontend_config.yaml : SQLALCHEMY_DATABASE_PATH : \"/home/francischen/Development/pslx/example/frontend_example/frontend_example.db\" CONTAINER_BACKEND_CONFIG : SERVER_URL : \"localhost:11443\" ROOT_CERTIFICATE_PATH : \"\" PROTO_VIEWER_CONFIG : SERVER_1 : SERVER_URL : \"localhost:11444\" ROOT_CERTIFICATE_PATH : \"\" FILE_VIEWER_CONFIG : SERVER_1 : SERVER_URL : \"localhost:11445\" ROOT_CERTIFICATE_PATH : \"\" LOGGING_QUEUE_CONFIG : EXCHANGE : \"prod.pubsub.pslx_dedicated_logging\" TOPIC : \"PSLX_DEDICATED_LOGGING\" CONNECTION_STR : \"amqp://guest:guest@localhost:5672\" USER_NAME : \"guest\" PASSWORD : \"guest\" The above user_name and password are the credentials used to login the frontend. In the second step, one needs to parse the yaml file into a FrontendConfig proto. One can follow this script defined in frontend_example/create_config.py : from pslx.util.common_util import CommonUtil from pslx.util.file_util import FileUtil def main (): yaml_path = \"example/frontend_example/frontend_config.yaml\" proto_path = \"example/frontend_example/frontend_config.pb\" config = CommonUtil . make_frontend_config ( yaml_path = yaml_path ) print ( config ) FileUtil . write_proto_to_file ( proto = config , file_name = proto_path ) if __name__ == \"__main__\" : main () Then, to enable login, please use the following snippet to create the table from pslx.micro_service.frontend import pslx_frontend_db if __name__ == '__main__' : pslx_frontend_db . create_all () , and the related command is PSLX_FRONTEND_CONFIG_PROTO_PATH = example/frontend_example/frontend_config.pb \\ PYTHONPATH = . python example/frontend_example/create_frontend_table.py Finally, one can launch the backend by pointing PSLX_FRONTEND_CONFIG_PROTO_PATH to the proto with from pslx.micro_service.frontend import pslx_frontend_ui_app if __name__ == \"__main__\" : pslx_frontend_ui_app . run ( host = 'localhost' , port = 5001 , debug = True ) The command is PSLX_FRONTEND_CONFIG_PROTO_PATH = example/frontend_example/frontend_config.pb \\ PYTHONPATH = . python example/frontend_example/run_frontend.py","title":"PSLX frontend"},{"location":"micro_services/instant_messaging/","text":"PSLX supports binding to Slack, Rocketchat and Microsoft Teams webhooks through RPC. The detailed implementation of the RPC can be found at instant_messaging/rpc.py . To start a RPC server, one can take a look at the example . The server's implementation is easy: from pslx.micro_service.instant_messaging.rpc import InstantMessagingRPC from pslx.micro_service.rpc.generic_server import GenericServer if __name__ == \"__main__\" : server_url = \"localhost:11443\" example_rpc = InstantMessagingRPC ( rpc_storage = None ) example_server = GenericServer ( server_name = 'example' ) example_server . create_server ( max_worker = 1 , server_url = server_url ) example_server . bind_rpc ( rpc = example_rpc ) example_server . start_server () , and the client examples are: from pslx.micro_service.instant_messaging.client import SlackClient , RocketchatClient , TeamsClient if __name__ == \"__main__\" : server_url = \"localhost:11443\" message = \"hello world\" slack_client = SlackClient ( channel_name = 'staging_test' , webhook_url = 'https://hooks.slack.com/services/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' , server_url = server_url ) slack_client . send_message ( message = message , is_test = False ) rocketchat_client = RocketchatClient ( channel_name = 'internal_msg_queue' , webhook_url = 'http://165.227.12.178:3000/hooks/' '7YDoDrqcsyHHqRtHd/6YnK7GbDgzit38DwhH2TppRG6turXNdJ24JsbPyJhy28E6JG' , server_url = server_url ) rocketchat_client . send_message ( message = message , is_test = False ) teams_client = TeamsClient ( channel_name = 'internal_msg_queue' , webhook_url = \"https://outlook.office.com/webhook/\" \"82c18bf1-2994-403d-888d-f11343682d72@ec60084f-c956-4c79-94a1-a3243cf2eea8/\" \"IncomingWebhook/4880528f42434ef7827809b2c403c7f6/623b9f83-e4f8-4ae2-946a-710a6d4e085e\" , server_url = server_url ) teams_client . send_message ( message = message , is_test = False ) If is_test is set True, no actual message will be sent over RPC.","title":"Instant Messaging"},{"location":"micro_services/message_queue/","text":"PSLX also implements a message queue API, the application needs to inherit queue_base.py , especially overriding the function of get_response_and_status_impl that takes a user defined request proto message to a user defined response message. If the queue does not need response, please return None as the response. An example of this implementation of a queue for instant messaging including its consumer (like the RPC implementation) is import requests from pslx.micro_service.message_queue.queue_base import QueueBase from pslx.micro_service.message_queue.generic_consumer import GenericConsumer from pslx.schema.enums_pb2 import Status from pslx.schema.rpc_pb2 import InstantMessagingRPCRequest from pslx.storage.partitioner_storage import DailyPartitionerStorage from pslx.util.env_util import EnvUtil from pslx.util.file_util import FileUtil from pslx.util.timezone_util import TimezoneUtil class SlackQueue ( QueueBase ): REQUEST_MESSAGE_TYPE = InstantMessagingRPCRequest def get_response_and_status_impl ( self , request ): header = { 'Content-Type' : \"application/x-www-form-urlencoded\" , 'Cache-Control' : \"no-cache\" , } slack_payload = \"payload={'text':'\" + request . message + \" \\n Current time is \" \\ + str ( TimezoneUtil . cur_time_in_pst ()) + \"'}\" status = Status . SUCCEEDED try : requests . post ( request . webhook_url , data = slack_payload , headers = header ) except Exception as err : self . _logger . error ( \"Slack failed to send message with err \" + str ( err )) status = Status . FAILED return None , status if __name__ == \"__main__\" : consumer = GenericConsumer ( connection_str = 'amqp://guest:guest@localhost:5672' ) partitioner_dir = FileUtil . join_paths_to_dir_with_mode ( root_dir = FileUtil . join_paths_to_dir ( root_dir = EnvUtil . get_pslx_env_variable ( var = 'PSLX_DATABASE' ), base_name = 'msg_queue' ), base_name = 'msg_queue_example' , ttl = '1h' ) storage = DailyPartitionerStorage () storage . initialize_from_dir ( dir_name = partitioner_dir ) slack_queue = SlackQueue ( queue_name = 'slack_queue' , queue_storage = storage ) consumer . bind_queue ( queue = slack_queue ) consumer . start_consumer () To use the message queue, one can also follow the example code for an example of producer: from pslx.micro_service.message_queue.producer_base import ProducerBase from pslx.schema.enums_pb2 import InstantMessagingType from pslx.schema.rpc_pb2 import InstantMessagingRPCRequest class SlackProducer ( ProducerBase ): def send_message ( self , channel_name , webhook_url , message ): request = InstantMessagingRPCRequest () request . is_test = False request . type = InstantMessagingType . SLACK request . channel_name = channel_name request . webhook_url = webhook_url request . message = message self . send_request ( request = request ) if __name__ == \"__main__\" : producer = SlackProducer ( queue_name = 'slack_queue' , connection_str = 'amqp://guest:guest@localhost:5672' ) producer . send_message ( channel_name = 'staging_test' , webhook_url = 'https://hooks.slack.com/services/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' , message = \"hello world\" ) One note is that in order to use the message queue, please install rabbitmq .","title":"Message Queue"},{"location":"micro_services/pubsub/","text":"PSLX implements the basic Pubsub model through Rabbitmq. The publisher is directly usable as all the basic building blocks are already implemented. In addition, here the message that goes through the pubsub model needs to be a proto message. Here is an example of publisher: import time from pslx.micro_service.pubsub.publisher import Publisher from pslx.util.timezone_util import TimeSleepObj from pslx.schema.rpc_pb2 import HealthCheckerRequest if __name__ == \"__main__\" : publisher1 = Publisher ( exchange_name = 'test_exchange_1' , topic_name = 'test1' , connection_str = 'amqp://guest:guest@localhost:5672' ) message1 = HealthCheckerRequest () message1 . server_url = 'test1' message1 . secure = False publisher2 = Publisher ( exchange_name = 'test_exchange_2' , topic_name = 'test2' , connection_str = 'amqp://guest:guest@localhost:5672' ) message2 = HealthCheckerRequest () message2 . server_url = 'test2' message2 . secure = True while True : print ( \"Publish message...\" ) publisher1 . publish ( message = message1 ) publisher2 . publish ( message = message2 ) time . sleep ( TimeSleepObj . ONE_SECOND ) For subscriber, the user needs to wrap it in an operator with the following function defined: pubsub_parse_message ( exchange_name , topic_name , message ) which takes in the exchange and topic (routing_key) of the message. The purpose of this function to allow user to have the access of handling the message. To bind the subscriber to the operator, one can use: bind_to_op ( op ) An example subscriber of the above publisher is from pslx.streaming.operator import StreamingOperator from pslx.streaming.container import DefaultStreamingContainer from pslx.micro_service.pubsub.subscriber_base import Subscriber from pslx.schema.rpc_pb2 import HealthCheckerRequest from pslx.util.dummy_util import DummyUtil class SubscriberExampleOp ( StreamingOperator ): def __init__ ( self ): super () . __init__ ( operator_name = 'subscriber_example_op' ) @staticmethod def pubsub_msg_parser ( exchange_name , topic_name , message ): print ( exchange_name , topic_name , message ) def execute_impl ( self ): subscriber = Subscriber ( connection_str = 'amqp://guest:guest@localhost:5672' ) subscriber . bind_to_op ( self ) subscriber . subscribe ( exchange_name = 'test_exchange_1' , topic_name = 'test1' , message_type = HealthCheckerRequest ) subscriber . subscribe ( exchange_name = 'test_exchange_2' , topic_name = 'test2' , message_type = HealthCheckerRequest ) subscriber . start () class SubscriberExampleContainer ( DefaultStreamingContainer ): def __init__ ( self ): super () . __init__ ( container_name = 'subscriber_example_container' , ttl = 7 ) if __name__ == \"__main__\" : op = SubscriberExampleOp () container = SubscriberExampleContainer () container . add_operator_edge ( from_operator = op , to_operator = DummyUtil . dummy_streaming_operator ()) container . add_operator_edge ( from_operator = op , to_operator = DummyUtil . dummy_streaming_operator ( operator_name = 'dummy_streaming_operator' ) ) container . initialize () container . execute ()","title":"Publisher/Subscriber"},{"location":"micro_services/rpc_storage_io/","text":"PSLX supports reading from any remote storage. The detailed RPC implementation is located at rpc_io/rpc.py , and all the storage types defined in storage are supported. One can also take a look at the example in example/rpc_io_example . Like other services, the server can be launched with following code: from pslx.micro_service.rpc_io.rpc import RPCIO from pslx.micro_service.rpc.generic_server import GenericServer from pslx.storage.partitioner_storage import MinutelyPartitionerStorage from pslx.util.env_util import EnvUtil from pslx.util.file_util import FileUtil if __name__ == \"__main__\" : server_url = \"localhost:11443\" partitioner_dir = FileUtil . join_paths_to_dir_with_mode ( root_dir = FileUtil . join_paths_to_dir ( root_dir = EnvUtil . get_pslx_env_variable ( var = 'PSLX_DATABASE' ), base_name = 'rpc_io' ), base_name = 'rpc_io_example' , ttl = 1 ) storage = MinutelyPartitionerStorage () storage . initialize_from_dir ( dir_name = partitioner_dir ) example_rpc = RPCIO ( rpc_storage = storage ) example_server = GenericServer ( server_name = 'example' ) example_server . create_server ( max_worker = 1 , server_url = server_url ) example_server . bind_rpc ( rpc = example_rpc ) example_server . start_server () Note here, we also use a RPC storage (Proto Table) for GenericRPCRequestResponsePair storage. The example clients are defined in rpc_io_example/client.py import datetime from pslx.micro_service.rpc_io.client import DefaultStorageRPC , FixedSizeStorageRPC , ProtoTableStorageRPC , \\ PartitionerStorageRPC , ShardedProtoTableStorageRPC from pslx.schema.enums_pb2 import PartitionerStorageType from pslx.schema.snapshots_pb2 import NodeSnapshot if __name__ == \"__main__\" : server_url = \"localhost:11443\" file_name = \"pslx/test/storage/test_data/test_default_storage_data.txt\" example_client = DefaultStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = file_name )) example_client = FixedSizeStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = file_name , params = { 'fixed_size' : 1 , 'num_line' : 2 , 'force_load' : True , } )) file_name = \"pslx/test/storage/test_data/test_proto_table_data.pb\" example_client = ProtoTableStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = file_name , params = { 'key' : 'test' , 'message_type' : NodeSnapshot , 'proto_module' : 'pslx.schema.snapshots_pb2' , } )) file_name = \"pslx/test/storage/test_data/test_proto_table_data.pb\" example_client = ProtoTableStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = file_name , params = { 'key' : 'test1' , 'message_type' : NodeSnapshot , 'proto_module' : 'pslx.schema.snapshots_pb2' , } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_1/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_1/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , 'read_oldest' : True , } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_3/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read_range ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , 'start_time' : datetime . datetime ( 2019 , 1 , 5 ), 'end_time' : datetime . datetime ( 2020 , 1 , 5 ), } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_4/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , 'is_proto_table' : True , } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_4/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , 'is_proto_table' : True , 'read_oldest' : True , } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_4/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read_range ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , 'start_time' : datetime . datetime ( 2019 , 1 , 5 ), 'end_time' : datetime . datetime ( 2020 , 1 , 5 ), 'is_proto_table' : True } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_5/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , 'is_proto_table' : True , 'base_name' : 'some_data.pb' , } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_5/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , 'is_proto_table' : True , 'read_oldest' : True , 'base_name' : 'some_data.pb' , } )) dir_name = \"pslx/test/storage/test_data/yearly_partitioner_5/\" example_client = PartitionerStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read_range ( file_or_dir_path = dir_name , params = { 'PartitionerStorageType' : PartitionerStorageType . YEARLY , 'start_time' : datetime . datetime ( 2019 , 1 , 5 ), 'end_time' : datetime . datetime ( 2020 , 1 , 5 ), 'is_proto_table' : True } )) dir_name = \"pslx/test/storage/test_data/sharded_proto_table_1\" example_client = ShardedProtoTableStorageRPC ( client_name = 'example_rpc_io' , server_url = server_url ) print ( example_client . read ( file_or_dir_path = dir_name , params = { 'keys' : [ 'test_0' , 'test_1' , 'test_100' ] } )) We can see that in addition to the required parameter for the storage, we also need to pass the following parameters: 1. PartitionerStorageType for PartitionerStorageRPC . 2. proto_module for ProtoTableStorageRPC if one wants the message to be deserialized to the desired format. As a side note, the DefaultStorageRPC will now ignore the parameter of num_line as in the RPC case, the function call will always return the content of the whole underlying file.","title":"RPC Storage IO"}]}